{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Funcionalidades para proyecto isw**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from urllib.request import *\n",
    "from urllib.parse import *\n",
    "from bs4 import BeautifulSoup\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def obtener_titulo(titulo_de_la_busqueda):\n",
    " \n",
    "    # Descargamos la información de la página.\n",
    "    soup_busqueda = BeautifulSoup(urlopen(\"http://www.fisicanet.com.ar/buscador/fis_buscador.php?criterio=\"+titulo_de_la_busqueda+\"&Buscar=Buscar&registros=1000&como=todas\"), 'html.parser')\n",
    " \n",
    "\n",
    "    lista_links=[]\n",
    "    #obtenemos la informacion para obtener los links de las busquedas \n",
    "    busqueda=soup_busqueda.find_all(\"div\",class_=\"brBot1\")\n",
    "    for i in busqueda:\n",
    "        print(\"i vale: \")\n",
    "        print(i)\n",
    "        for j in i.find_all('a'):\n",
    "            lista_links.append(j['href'])\n",
    "            # se agregan en una lista los url de las paginas de la busqueda hecha\n",
    "    contador=1\n",
    "    print(\"Entro al archivo  \"+str(contador))\n",
    "    #for en el que va entrando pagina por pagina extrañendo imagenes y texto\n",
    "    for links in lista_links:\n",
    "        if contador>40:\n",
    "            break\n",
    "        #se busca la informacion en relacion a la pagina\n",
    "        sopa_de_cada_busqueda=BeautifulSoup(urlopen(links), 'html.parser')\n",
    "        #extraemos el texto sobre la materia\n",
    "        texto=sopa_de_cada_busqueda.find(itemprop=\"articleBody\").get_text()\n",
    "        archivo=open(\"./Datos/texto\"+str(contador)+\".txt\",'w',encoding=\"utf-8\")\n",
    "        archivo.write(str(texto))\n",
    "        archivo.close()\n",
    "        #agregamos el texto en un archivo txt\n",
    "        contador+=1\n",
    "        #otenemos las imagenes de la pagina\n",
    "        imagenes=sopa_de_cada_busqueda.find(itemprop=\"articleBody\").find_all(\"img\")\n",
    "        tupla = links.split(\"/\") \n",
    "        tupla_link= tupla[0]+\"//\"+tupla[2]+\"/\"+tupla[3]+\"/\"+tupla[4]+\"/\"\n",
    "        print(\"lego a imagen for  \"+str(contador))\n",
    "        #entra al for para extraer las imaagenes\n",
    "        for imagen in imagenes:\n",
    "             #se crea el link que nos llevara a la imagen en el buscador web\n",
    "             link= urljoin(links,imagen['src'])\n",
    "             print(link)\n",
    "             print(links)\n",
    "             if \"img/\" in link:\n",
    "                 break\n",
    "             #se descarga la imagen\n",
    "             urlretrieve(link, './Datos/'+os.path.basename(link))\n",
    "        print(\"paso for imagen  \"+str(contador))\n",
    "    return 1\n",
    "\n",
    "titulo_busqueda=input()\n",
    "\n",
    "obtener_titulo(titulo_busqueda)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Trabajo de texto, análisis del texto* \n",
    "\n",
    "https://www.youtube.com/user/victorlavrenko canal explicando de RNA\n",
    "\n",
    "http://laboratorios.fi.uba.ar/lsi/bertona-tesisingenieriainformatica.pdf pdf conceptos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from scipy import sparse\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "\n",
    "class TextClassifier():\n",
    "\n",
    "    \"\"\"Clasificador automático de textos.\n",
    "\n",
    "    Usa TF-IDF para vectorizar.\n",
    "    Usa SVM para clasificar.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, texts, ids, vocabulary=None, encoding='utf-8'):\n",
    "        \"\"\"Definido en la declaracion de la clase.\n",
    "\n",
    "        Attributes:\n",
    "            texts (list of str): Textos a clasificar.\n",
    "            ids (list of str): Identificadores únicos para cada texto (debe\n",
    "                tener la misma longitud que `texts`).\n",
    "            vocabulary (list): Opcional. Vocabulario a tener en cuenta para la\n",
    "                vectorización de los textos. Default: usa todas las palabras\n",
    "                presentes en los textos, salvo los ES_stopwords.txt.\n",
    "            encoding (str): Codificación de los textos en `texts` y en `ids`.\n",
    "        \"\"\"\n",
    "        this_dir, this_filename = os.path.split('__file__')\n",
    "        es_stopwords = pd.read_csv(os.path.join(this_dir, 'ES_stopwords.txt'),\n",
    "                                   header=None, encoding='utf-8')\n",
    "        es_stopwords = list(np.squeeze(es_stopwords.values))\n",
    "        self._check_id_length(ids)\n",
    "        self.vectorizer = CountVectorizer(\n",
    "            input='content', encoding=encoding, decode_error='strict',\n",
    "            strip_accents='ascii', lowercase=True, preprocessor=None,\n",
    "            tokenizer=None, stop_words=es_stopwords, ngram_range=(1, 1),\n",
    "            analyzer='word', max_df=1.0, min_df=1, max_features=None,\n",
    "            vocabulary=vocabulary, binary=False)\n",
    "\n",
    "        self.transformer = TfidfTransformer()\n",
    "        self.ids = None # Matiene una lista ordenada de ids de textos.\n",
    "        self.term_mat = None  # Matriz que cuenta los terminos en un texto.\n",
    "        self.tfidf_mat = None  # Matriz de relevancia de los terminos.\n",
    "        self.reload_texts(texts, ids)\n",
    "\n",
    "    def __str__(self):\n",
    "        \"\"\"Representacion en str del objeto.\"\"\"\n",
    "        string = \"\"\" Clasificador de textos con {:d} textos almacenados, \\\n",
    "        \"\"\".format(len(self.ids))\n",
    "        return string\n",
    "\n",
    "    def make_classifier(self, name, ids, labels):\n",
    "        \"\"\"Entrenar un clasificador SVM sobre los textos cargados.\n",
    "\n",
    "        Args:\n",
    "            name (str): Nombre para el clasidicador.\n",
    "            ids (list): Se espera una lista de N ids de textos ya almacenados\n",
    "                en el TextClassifier.\n",
    "            labels (list): Se espera una lista de N etiquetas. Una por cada id\n",
    "                de texto presente en ids.\n",
    "        Nota:\n",
    "            Usa el clasificador de `Scikit-learn <http://scikit-learn.org/>`_\n",
    "        \"\"\"\n",
    "        if not all(np.in1d(ids, self.ids)):\n",
    "            raise ValueError(\"Hay ids de textos que no se encuentran \\\n",
    "                              almacenados.\")\n",
    "        setattr(self, name, SGDClassifier())\n",
    "        classifier = getattr(self, name)\n",
    "        indices = np.searchsorted(self.ids, ids)\n",
    "        classifier.fit(self.tfidf_mat[indices, :], labels)\n",
    "\n",
    "    def retrain(self, name, ids, labels):\n",
    "        \"\"\"Reentrenar parcialmente un clasificador SVM.\n",
    "\n",
    "        Args:\n",
    "            name (str): Nombre para el clasidicador.\n",
    "            ids (list): Se espera una lista de N ids de textos ya almacenados\n",
    "                en el TextClassifier.\n",
    "            labels (list): Se espera una lista de N etiquetas. Una por cada id\n",
    "                de texto presente en ids.\n",
    "        Nota:\n",
    "            Usa el clasificador de `Scikit-learn <http://scikit-learn.org/>`_\n",
    "        \"\"\"\n",
    "        if not all(np.in1d(ids, self.ids)):\n",
    "            raise ValueError(\"Hay ids de textos que no se encuentran \\\n",
    "                              almacenados.\")\n",
    "        try:\n",
    "            classifier = getattr(self, name)\n",
    "        except AttributeError:\n",
    "            raise AttributeError(\"No hay ningun clasificador con ese nombre.\")\n",
    "        indices = np.in1d(self.ids, ids)\n",
    "        if isinstance(labels, str):\n",
    "            labels = [labels]\n",
    "        classifier.partial_fit(self.tfidf_mat[indices, :], labels)\n",
    "\n",
    "    def classify(self, classifier_name, examples, max_labels=None,\n",
    "                 goodness_of_fit=False):\n",
    "        \"\"\"Usar un clasificador SVM para etiquetar textos nuevos.\n",
    "\n",
    "        Args:\n",
    "            classifier_name (str): Nombre del clasidicador a usar.\n",
    "            examples (list or str): Se espera un ejemplo o una lista de\n",
    "                ejemplos a clasificar en texto plano o en ids.\n",
    "            max_labels (int, optional): Cantidad de etiquetas a devolver para\n",
    "                cada ejemplo. Si se devuelve mas de una el orden corresponde a\n",
    "                la plausibilidad de cada etiqueta. Si es None devuelve todas\n",
    "                las etiquetas posibles.\n",
    "            goodness_of_fit (bool, optional): Indica si devuelve o no una\n",
    "                medida de cuan buenas son las etiquetas.\n",
    "        Nota:\n",
    "            Usa el clasificador de `Scikit-learn <http://scikit-learn.org/>`_\n",
    "        \"\"\"\n",
    "        classifier = getattr(self, classifier_name)\n",
    "        texts_vectors = self._make_text_vectors(examples)\n",
    "        return classifier.classes_, classifier.decision_function(texts_vectors)\n",
    "\n",
    "    def _make_text_vectors(self, examples):\n",
    "        \"\"\"Funcion para generar los vectores tf-idf de una lista de textos.\n",
    "\n",
    "        Args:\n",
    "            examples (list or str): Se espera un ejemplo o una lista de:\n",
    "                o bien ids, o bien textos.\n",
    "        Returns:\n",
    "            textvec (sparse matrix): Devuelve una matriz sparse que contiene\n",
    "                los vectores TF-IDF para los ejemplos que se pasan de entrada.\n",
    "                El tamaño de la matriz es de (N, T) donde N es la cantidad de\n",
    "                ejemplos y T es la cantidad de términos en el vocabulario.\n",
    "        \"\"\"\n",
    "        if isinstance(examples, str):\n",
    "            if examples in self.ids:\n",
    "                textvec = self.tfidf_mat[self.ids == examples, :]\n",
    "            else:\n",
    "                textvec = self.vectorizer.transform([examples])\n",
    "                textvec = self.transformer.transform(textvec)\n",
    "        elif type(examples) is list:\n",
    "            if all(np.in1d(examples, self.ids)):\n",
    "                textvec = self.tfidf_mat[np.in1d(self.ids, examples)]\n",
    "            elif not any(np.in1d(examples, self.ids)):\n",
    "                textvec = self.vectorizer.transform(examples)\n",
    "                textvec = self.transformer.transform(textvec)\n",
    "            else:\n",
    "                raise ValueError(\"Las listas de ejemplos deben ser todos ids\\\n",
    "                                  de textos almacenados o todos textos planos\")\n",
    "        else:\n",
    "            raise TypeError(\"Los ejemplos no son del tipo de dato adecuado.\")\n",
    "        return textvec\n",
    "\n",
    "    def get_similar(self, example, max_similars=3, similarity_cutoff=None,\n",
    "                    term_diff_cutoff=0.6):\n",
    "        \"\"\"Devuelve textos similares al ejemplo dentro de los textos entrenados.\n",
    "\n",
    "        Nota:\n",
    "            Usa la distancia de coseno del vector de features TF-IDF\n",
    "\n",
    "        Args:\n",
    "            example (str): Se espera un id de texto o un texto a partir del\n",
    "                cual se buscaran otros textos similares.\n",
    "            max_similars (int, optional): Cantidad de textos similares a\n",
    "                devolver.\n",
    "            similarity_cutoff (float, optional): Valor umbral de similaridad\n",
    "                para definir que dos textos son similares entre si.\n",
    "            term_diff_cutoff (float, optional): Este valor sirve para controlar\n",
    "                el umbral con el que los terminos son considerados importantes\n",
    "                a la hora de recuperar textos (no afecta el funcionamiento de\n",
    "                que textos se consideran cercanos, solo la cantidad de terminos\n",
    "                que se devuelven en best_words).\n",
    "\n",
    "        Returns:\n",
    "            text_ids (list of str): Devuelve los ids de los textos sugeridos.\n",
    "            sorted_dist (list of float): Devuelve la distancia entre las\n",
    "                opciones sugeridas y el ejemplo dado como entrada.\n",
    "            best_words (list of list): Para cada sugerencia devuelve las\n",
    "                palabras mas relevantes que se usaron para seleccionar esa\n",
    "                sugerencia.\n",
    "        \"\"\"\n",
    "        if max_similars > self.term_mat.shape[0]:\n",
    "            raise ValueError(\"No se pueden pedir mas sugerencias que la \\\n",
    "                              cantidad de textos que hay almacenados.\")\n",
    "        if example in self.ids:\n",
    "            index = self.ids == example\n",
    "            exmpl_vec = self.tfidf_mat[index, :]\n",
    "            distances = np.squeeze(pairwise_distances(self.tfidf_mat,\n",
    "                                                      exmpl_vec))\n",
    "            # Pongo la distancia a si mismo como inf, par que no se devuelva a\n",
    "            # si mismo como una opcion\n",
    "            distances[index] = np.inf\n",
    "        else:\n",
    "            exmpl_vec = self.vectorizer.transform([example])  # contar terminos\n",
    "            exmpl_vec = self.transformer.transform(exmpl_vec)  # calcular tfidf\n",
    "            distances = np.squeeze(pairwise_distances(self.tfidf_mat,\n",
    "                                                      exmpl_vec))\n",
    "        sorted_indices = np.argsort(distances)\n",
    "        closest_n = sorted_indices[:max_similars]\n",
    "        sorted_dist = distances[closest_n]\n",
    "        if similarity_cutoff:\n",
    "            closest_n = closest_n[sorted_dist < similarity_cutoff]\n",
    "            sorted_dist = sorted_dist[sorted_dist < similarity_cutoff]\n",
    "        best_words = []\n",
    "        exmpl_vec = exmpl_vec.toarray()\n",
    "        for suggested in closest_n:\n",
    "            test_vec = self.tfidf_mat[suggested,:].toarray()\n",
    "            differences = np.abs(exmpl_vec - test_vec)**2 / \\\n",
    "                            (exmpl_vec**2 + test_vec**2)\n",
    "            differences = np.squeeze(np.array(differences))\n",
    "            sort_I = np.argsort(differences)\n",
    "            limit = np.flatnonzero((differences[sort_I] > term_diff_cutoff) \\\n",
    "                                   | (np.isnan(differences[sort_I]))\n",
    "                                   )[0]\n",
    "            best_words.append([k for k,v in\n",
    "                               self.vectorizer.vocabulary_.items()\n",
    "                                if v in sort_I[:limit]])\n",
    "        text_ids = self.ids[closest_n]\n",
    "        return list(text_ids), list(sorted_dist), best_words\n",
    "\n",
    "    def reload_texts(self, texts, ids, vocabulary=None):\n",
    "        \"\"\"Calcula los vectores de terminos de textos y los almacena.\n",
    "\n",
    "        A diferencia de :func:`~TextClassifier.TextClassifier.store_text` esta\n",
    "        funcion borra cualquier informacion almacenada y comienza el conteo\n",
    "        desde cero. Se usa para redefinir el vocabulario sobre el que se\n",
    "        construyen los vectores.\n",
    "\n",
    "        Args:\n",
    "            texts (list): Una lista de N textos a incorporar.\n",
    "            ids (list): Una lista de N ids alfanumericos para los textos.\n",
    "        \"\"\"\n",
    "        self._check_id_length(ids)\n",
    "        self.ids = np.array(sorted(ids))\n",
    "        if vocabulary:\n",
    "            self.vectorizer.vocabulary = vocabulary\n",
    "        sorted_texts = [x for (y,x) in sorted(zip(ids,texts))]\n",
    "        self.term_mat = self.vectorizer.fit_transform(sorted_texts)\n",
    "        self._update_tfidf()\n",
    "\n",
    "    # def store_text(self, texts, ids, replace_texts=False):\n",
    "    #     \"\"\"Calcula los vectores de terminos de un texto y los almacena.\n",
    "    #         NOT IMPLEMENTED.\n",
    "    #     Nota:\n",
    "    #         Esta funcion usa el vocabulario que ya esta almacenado, es decir,\n",
    "    #         que no se incorporan nuevos terminos. Si se quiere cambiar el\n",
    "    #         vocabulario deben recargarse todos los textos con\n",
    "    #         :func:`~TextClassifier.TextClassifier.reload_texts`\n",
    "    #     Args:\n",
    "    #         texts (list): Una lista de N textos a incorporar.\n",
    "    #         ids (list of str): Una lista de N ids alfanumericos para los textos\n",
    "    #         replace_texts (bool, optional): Indica si deben reemplazarse los\n",
    "    #             textos cuyo id ya este almacenado. Si es False y algun id ya se\n",
    "    #             encuentra almacenado se considera un error.\n",
    "    #     \"\"\"\n",
    "    #     self._check_id_length(ids)\n",
    "    #     if not replace_texts and any(np.in1d(ids, self.ids)):\n",
    "    #         raise ValueError(\"Alguno de los ids provistos ya esta en el \\\n",
    "    #                           indice\")\n",
    "    #     else:\n",
    "    #         ids = np.array(ids)\n",
    "    #         partial_mat = self.vectorizer.transform(texts)\n",
    "    #         # Si no hay ids ya guardados solo concateno y los agrego al\n",
    "    #         # array self.ids\n",
    "    #         if not any(np.in1d(ids, self.ids)):\n",
    "    #             self.ids = np.r_[self.ids, ids]\n",
    "    #             self.term_mat = sparse.vstack((self.term_mat,\n",
    "    #                                            partial_mat))\n",
    "    #         # Si los hay,\n",
    "    #         else:\n",
    "    #             oldrows = np.in1d(self.ids, ids)\n",
    "    #             oldpartial = np.in1d(ids, self.ids)\n",
    "    #             # Actualizo las filas que ya estaban\n",
    "    #             self.term_mat[oldrows, :] = partial_mat[oldpartial, :]\n",
    "    #             # y agrego las que no\n",
    "    #             partial_mat = partial_mat[~oldpartial, :]\n",
    "    #             self.term_mat = sparse.vstack((self.term_mat, partial_mat))\n",
    "    #             # concateno los viejos ids y los nuevos\n",
    "    #             self.ids = np.r_[self.ids, ids[~oldpartial]]\n",
    "    #     self._update_tfidf()\n",
    "\n",
    "    def _update_tfidf(self):\n",
    "        self.tfidf_mat = self.transformer.fit_transform(self.term_mat)\n",
    "\n",
    "    def _check_id_length(self, ids):\n",
    "        if any([len(x) > 10 for x in ids]):\n",
    "            warnings.warn(\"Hay ids que son muy largos. Es posible que se hayan \\\n",
    "            ingresado textos planos en lugar de ids.\")\n",
    "\n",
    "    def _check_repeated_ids(self, ids):\n",
    "        if length(np.unique(ids)) != length(ids):\n",
    "            raise ValueError(\"Hay ids repetidos.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "list_of_files = glob.glob('./Datos/*.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = []\n",
    "for fileName in list_of_files:\n",
    "    data_list = open( fileName, \"r\" ,encoding=\"utf8\").read()\n",
    "    break\n",
    "data.append(data_list)\n",
    "print(data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
